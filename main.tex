\documentclass[12pt, a4paper]{article}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{caption}

% Page Margins
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}

% Code Styling
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegray},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- TITLE PAGE ---
\title{
    \vspace{2cm}
    \textbf{\Large Comparative Analysis of Logistic Regression and Multi-Layer Perceptrons for Image Classification} \\
    \vspace{1cm}
    \large Artificial Intelligence Lab Assignment
}
\author{
    \textbf{Devesh Singh Chauhan} \\
    M.Sc. Mathematics, Semester VI \\
    SVNIT, Surat
}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}
\newpage

\setcounter{page}{1}

\section{Introduction}
This report presents a comparative analysis of linear and non-linear approaches to image classification using the Kaggle Clothes Dataset. The objective is to predict clothing types from images using Logistic Regression and Multi-Layer Perceptrons (MLP). We specifically investigate the impact of network depth (number of hidden layers) and non-linearity (activation functions) on model performance, evaluated using the Macro F1 score to account for class balance.

\section{Methodology}

\subsection{Data Preprocessing}
The dataset consists of 7,500 images across 15 classes (e.g., Blazer, Jeans, Hoodie). To prepare the data for the models:
\begin{itemize}
    \item \textbf{Resizing:} Images were resized to $64 \times 64$ pixels.
    \item \textbf{Flattening:} The 2-D image arrays were flattened into 1-D vectors $v \in \mathbb{R}^{12288}$.
    \item \textbf{Normalization:} Pixel intensity values were standardized using z-score normalization ($z = \frac{x - \mu}{\sigma}$) to ensure stable gradient descent convergence.
    \item \textbf{Splitting:} A stratified 80:20 train-test split was used (6,000 training images, 1,500 test images).
\end{itemize}

\subsection{Model Architectures}
\begin{enumerate}
    \item \textbf{Logistic Regression:} A linear model optimized using the LBFGS solver with a multinomial loss function.
    \item \textbf{Multi-Layer Perceptron (MLP):} A feed-forward neural network implemented in PyTorch with:
    \begin{itemize}
        \item \textbf{Input Layer:} 12,288 neurons.
        \item \textbf{Hidden Layers:} Variants with 1, 2, and 3 layers (128 neurons each).
        \item \textbf{Activation Functions:} Variants using ReLU, Sigmoid, and Tanh.
    \end{itemize}
\end{enumerate}

\section{Results}

\subsection{Logistic Regression Performance}
The baseline Logistic Regression model achieved the following performance:
\begin{itemize}
    \item \textbf{Macro F1 Score:} 0.3014
\end{itemize}
While computationally efficient, the linear nature of the model limits its ability to capture complex spatial hierarchies. The classification report indicated that distinct classes like 'Jeans' had higher recall (0.54), whereas visually similar classes like 'Blazer' performed poorly (0.13).

\subsection{MLP Experimental Analysis}
We performed two experiments to optimize the neural network architecture.

\subsubsection{Experiment 1: Effect of Hidden Layers}
Using the ReLU activation function, we varied the depth of the network. A consistent improvement was observed as network depth increased.

\begin{table}[H]
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Number of Hidden Layers} & \textbf{Macro F1 Score} \\
        \midrule
        1 Layer & 0.3794 \\
        2 Layers & 0.3817 \\
        3 Layers & 0.3987 \\
        \bottomrule
    \end{tabular}
    \caption{Performance variation with network depth (ReLU).}
\end{table}

\subsubsection{Experiment 2: Effect of Activation Functions}
Using a fixed depth of 2 hidden layers, we compared three activation functions.

\begin{table}[H]
    \centering
    \begin{tabular}{cc}
        \toprule
        \textbf{Activation Function} & \textbf{Macro F1 Score} \\
        \midrule
        ReLU & 0.3897 \\
        Sigmoid & 0.3189 \\
        Tanh & 0.2867 \\
        \bottomrule
    \end{tabular}
    \caption{Performance variation with activation functions.}
\end{table}

\subsection{Comparative Visualization}
The following plot summarizes the impact of hyperparameters on model performance.

\begin{figure}[H]
    \centering
    % Ensure mlp_comparison.png is in the same folder as this .tex file
    \includegraphics[width=1.0\linewidth]{mlp_comparison.png}
    \caption{Impact of Hidden Layers (Left) and Activation Functions (Right) on Macro F1 Score.}
    \label{fig:comparison}
\end{figure}

\section{Discussion & Conclusion}
The comparative analysis demonstrates that the \textbf{Multi-Layer Perceptron outperforms the linear baseline} (0.3987 vs 0.3014).

\begin{itemize}
    \item \textbf{Best Architecture:} The 3-Layer MLP with ReLU activation achieved the highest Macro F1 score of \textbf{0.3987}.
    \item \textbf{Activation Functions:} ReLU significantly outperformed Sigmoid and Tanh. Tanh performed surprisingly poorly (0.2867), even falling below the Logistic Regression baseline, likely due to vanishing gradients in the saturated regions of the activation function during training.
    \item \textbf{Network Depth:} Adding hidden layers provided marginal but consistent gains, suggesting the model benefits from the increased capacity to learn non-linear features.
\end{itemize}

\newpage
\appendix
\section{Python Code Implementation}
The following Python script was used to generate the results.

\begin{lstlisting}[language=Python]
import os
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score
from sklearn.preprocessing import StandardScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# --- Configuration ---
DATA_DIR = './clothes_dataset'
IMG_SIZE = (64, 64)
BATCH_SIZE = 32
EPOCHS = 20
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- Data Loading ---
def load_data(data_dir):
    images, labels = [], []
    classes = sorted(os.listdir(data_dir))
    class_map = {cls: i for i, cls in enumerate(classes)}
    for cls, idx in class_map.items():
        for img_name in os.listdir(os.path.join(data_dir, cls)):
            try:
                img = Image.open(os.path.join(data_dir, cls, img_name))
                img = img.resize(IMG_SIZE).convert('RGB')
                images.append(np.array(img))
                labels.append(idx)
            except: pass
    return np.array(images), np.array(labels), class_map

X, y, class_map = load_data(DATA_DIR)
X_flat = X.reshape(X.shape[0], -1) 
X_train, X_test, y_train, y_test = train_test_split(
    X_flat, y, test_size=0.2, stratify=y
)

# --- Normalization ---
scaler = StandardScaler()
X_train_norm = scaler.fit_transform(X_train)
X_test_norm = scaler.transform(X_test)

# --- Task 1: Logistic Regression ---
# Note: Using lbfgs solver for multinomial loss
log_reg = LogisticRegression(max_iter=1000, solver='lbfgs')
log_reg.fit(X_train_norm, y_train)
y_pred = log_reg.predict(X_test_norm)
print(f"LogReg F1: {f1_score(y_test, y_pred, average='macro')}")

# --- Task 2: MLP ---
X_train_t = torch.FloatTensor(X_train_norm).to(DEVICE)
y_train_t = torch.LongTensor(y_train).to(DEVICE)
train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32, shuffle=True)

class DynamicMLP(nn.Module):
    def __init__(self, in_dim, out_dim, layers, act_fn):
        super().__init__()
        net = []
        curr_dim = in_dim
        for _ in range(layers):
            net.append(nn.Linear(curr_dim, 128))
            net.append(act_fn())
            curr_dim = 128
        net.append(nn.Linear(curr_dim, out_dim))
        self.net = nn.Sequential(*net)
    def forward(self, x): return self.net(x)

# (Training loop omitted for brevity, full logic used in experiment)
\end{lstlisting}

\end{document}